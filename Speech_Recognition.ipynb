{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pOyL140v7PdZ",
    "outputId": "a417dd2a-35df-4663-98bb-d32f37e1d293"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.7.1+cu101 in /usr/local/lib/python3.6/dist-packages (1.7.1+cu101)\n",
      "Requirement already satisfied: torchvision==0.8.2+cu101 in /usr/local/lib/python3.6/dist-packages (0.8.2+cu101)\n",
      "Requirement already satisfied: torchaudio===0.7.2 in /usr/local/lib/python3.6/dist-packages (0.7.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (1.19.5)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (0.8)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (3.7.4.3)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.8.2+cu101) (7.0.0)\n"
     ]
    }
   ],
   "source": [
    "pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_l9kjtbI8puM",
    "outputId": "73d049f7-b9dd-4c55-8445-f74b9ed63601"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  '\"sox\" backend is being deprecated. '\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXBVKrYu7Pdg"
   },
   "source": [
    "### Transforming text to list of integers, and a list of integers to text\n",
    "0: \" ' \" <br>\n",
    "1: \"&nbsp; &nbsp; &nbsp;\" <br>\n",
    "2: \" a \" <br>\n",
    "3: \" b \" <br>\n",
    "... <br>\n",
    "26: \" y \" <br>\n",
    "27: \" z \" <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zgdPGwa17Pdh"
   },
   "outputs": [],
   "source": [
    "class TextTransform:\n",
    "\n",
    "    \"\"\"\n",
    "    Maps characters to integers and vice versa.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.index_map = {0:\"'\", 1:\" \"}\n",
    "        for ascii_code in range(97, 123):\n",
    "            self.index_map[ascii_code-95] = chr(ascii_code)\n",
    "            \n",
    "        self.char_map = {value:key for key, value in self.index_map.items()}\n",
    "\n",
    "    def text_to_int(self, text):\n",
    "\n",
    "        \"\"\"\n",
    "        Use a character map and convert text to an integer sequence.\n",
    "        \"\"\"\n",
    "\n",
    "        return [self.char_map[c] for c in text]\n",
    "\n",
    "    def int_to_text(self, labels):\n",
    "\n",
    "        \"\"\"\n",
    "        Use a label map and convert integer labels to a text sequence.\n",
    "        \"\"\"\n",
    "\n",
    "        return \"\".join([self.index_map[i] for i in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1JuTXow7Pdi"
   },
   "source": [
    "### Data preprocessing pipeline\n",
    "Converting signal to corresponding Mel Spectrogram, and labels to list of integers. Data augmentation techniques are added for the signal in training set, using frequency masking and time masking. \n",
    "\n",
    "Frequency masking and time masking are methods to augment speech data. They work by cuttingout random blocks of consecutive time and frequency dimension from the spectogram, as suggested \n",
    "in the paper [SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/pdf/1904.08779.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "D-7lwDdV7Pdi"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Object to transform audio for training.\n",
    "\"\"\"\n",
    "\n",
    "training_audio_transform = nn.Sequential(torchaudio.transforms.MelSpectrogram(sample_rate = 16000, n_mels = 128),\n",
    "                                         torchaudio.transforms.FrequencyMasking(freq_mask_param = 30),\n",
    "                                         torchaudio.transforms.TimeMasking(time_mask_param = 100))\n",
    "\n",
    "\"\"\"\n",
    "Object to transform audio for validation. \n",
    "\"\"\"\n",
    "\n",
    "validation_audio_transform = torchaudio.transforms.MelSpectrogram()\n",
    "\n",
    "\"\"\"\n",
    "Object for text transformation using the class we created above.\n",
    "\"\"\"\n",
    "\n",
    "text_transform = TextTransform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSDTadh47Pdi"
   },
   "source": [
    "### Collate function for the data loaders\n",
    "\n",
    "Signal is first converted to its Mel spectrogram. The spectrogram is returned in the shape (channel, n_mels, time). We'll need to pad the time dimension to make all the tensors of equal length. We use torch.nn.utils.rnn.pad_sequence function for it. However, for that we need the first dimension of each tensor to be that of time. To get the tensors to be in that shape, we squeeze the channel dimension of the spectrogram, which is 1, and take the transpose between n_mels and time. Now, we can feed the list of tensors in the pad_sequence function. A tensor of shape (batch, time, n_mels) is returned. We unsqueeze the channel dimension, because we need to feed this tensor to a CNN, getting a tensor of shape (batch, channels, time, n_mels) and then we take a transpose between time and n_mels dimensions.\n",
    "\n",
    "(channel, n_mels, time) $\\Rightarrow$ (n_mels, time) $\\Rightarrow$ (time, n_mels) $\\Rightarrow$ (batch, time, n_mels) $\\Rightarrow$ (batch, channel, time, n_mels) $\\Rightarrow$ (batch, channel, n_mels, time)\n",
    "\n",
    "Labels are converted to list of integers using the TextTransform class. The labels are padded to convert the batch into a tensor as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "T7FYvfzl7Pdj"
   },
   "outputs": [],
   "source": [
    "def data_processing(data, data_type = \"train\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    Collate function for the data loaders. Produces the Mel spectrograms of the signal.\n",
    "    \"\"\"\n",
    "\n",
    "    specs, labels, input_lengths, label_lengths = [], [], [], []\n",
    "    \n",
    "    for signal, _, text, _, _, _ in data:\n",
    "        \n",
    "        if data_type == \"train\":\n",
    "            ms = training_audio_transform(signal).squeeze(0).transpose(0, 1) \n",
    "        elif data_type == \"validation\":\n",
    "            ms = validation_audio_transform(signal).squeeze(0).transpose(0, 1)\n",
    "        else:\n",
    "            raise Exception(\"data_type parameter should be 'train' or 'validation'\")\n",
    "            \n",
    "        specs.append(ms)\n",
    "        input_lengths.append(ms.shape[0] // 2)\n",
    "        \n",
    "        label = torch.Tensor(text_transform.text_to_int(text.lower()))\n",
    "        labels.append(label)\n",
    "        label_lengths.append(len(label))\n",
    "\n",
    "    specs = pad_sequence(specs, batch_first = True).unsqueeze(1).transpose(2, 3)\n",
    "    labels = pad_sequence(labels, batch_first = True)\n",
    "\n",
    "    return specs, labels, input_lengths, label_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sHFpvEa7Pdj"
   },
   "source": [
    "### Model architecture\n",
    "\n",
    "The first layer in the model is that of a CNN with kernel size 3, and output number of channels equal to n_channels.\n",
    "\n",
    "A unit of RCNN layers follows the CNN layer. This unit is inspired by the paper [Identity Mappings in Deep Residual Networks](https://arxiv.org/pdf/1603.05027.pdf). The paper suggests to \"view the activation functions (ReLU and BN) as 'pre-activation' of the weight layers, in contrast to conventional wisdom of 'post-activation'.\" In this model, however, we experiment with layer normalization instead of batch normalization, and with GELU (Gaussian Error Linear Units) instead of ReLU. The paper [Gaussian Error Linear Units (GELUs)](https://arxiv.org/pdf/1606.08415.pdf) reports better performance of the GELU linearity when experimented against ReLU.\n",
    "\n",
    "The output of the RCNN is then fed into a fully connected layer, with output size equal to rnn_dim.\n",
    "\n",
    "Then follows the bidirectional GRU unit. Since we need real time transcribing, we use a GRU, instead of an LSTM, because of the lower number of parameters needed to be trained in a GRU. RNNs are known for their ability to capture temporal patterns. The bidirectional nature of the layers ensures that the learning is dependent upon, both, the past and the future elements of the sequence.\n",
    "\n",
    "Finally, we have the classifier unit, which essentially comprises of two fully connected layers, along with dropout and gelu activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1CZFeY-G7Pdk"
   },
   "outputs": [],
   "source": [
    "class CNNLayerNorm(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Layer normalization built for CNNs input.\n",
    "    \n",
    "    We need a custom class because the normalization needs to be carried out over the second last \n",
    "    dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_features):\n",
    "\n",
    "        super(CNNLayerNorm, self).__init__()\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape = n_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        \"\"\"\n",
    "        When passed into CNN, the input is of shape (batch_size, number_of channels, features, time).\n",
    "        \n",
    "        We want to normalize each feature, and so, we take a transpose, followed by normalization, \n",
    "        followed by another transpose.\n",
    "        \"\"\"\n",
    "        \n",
    "        x = x.transpose(2, 3).contiguous() \n",
    "        x = self.layer_norm(x)\n",
    "        x = x.transpose(2, 3).contiguous()\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualCNN(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Residual CNN unit, as inspired by the paper 'Identity Mappings in Deep Residual Networks'.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, dropout_rate, n_features):\n",
    "        \n",
    "        super(ResidualCNN, self).__init__()\n",
    "        \n",
    "        self.cnn1 = nn.Conv2d(in_channels = in_channels, \n",
    "                              out_channels = out_channels, \n",
    "                              kernel_size = kernel_size, \n",
    "                              stride = stride, \n",
    "                              padding = kernel_size // 2)\n",
    "        \n",
    "        self.cnn2 = nn.Conv2d(in_channels = out_channels, \n",
    "                              out_channels = out_channels, \n",
    "                              kernel_size = kernel_size, \n",
    "                              stride = stride, \n",
    "                              padding = kernel_size // 2)\n",
    "        \n",
    "        self.layer_norm = CNNLayerNorm(n_features = n_features)\n",
    "\n",
    "        self.gelu = nn.GELU()\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        residual = x  \n",
    "        x = self.dropout(self.gelu(self.layer_norm(x)))\n",
    "        x = self.cnn1(x)\n",
    "        x = self.dropout(self.gelu(self.layer_norm(x)))\n",
    "        x = self.cnn2(x)\n",
    "        x += residual\n",
    "        \n",
    "        return x \n",
    "\n",
    "\n",
    "class BidirectionalGRU(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    The Bidirectional Gated Recurrent Unit.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rnn_dim, hidden_size, dropout_rate, batch_first):\n",
    "        \n",
    "        super(BidirectionalGRU, self).__init__()\n",
    "        \n",
    "        self.BiGRU = nn.GRU(input_size = rnn_dim, \n",
    "                            hidden_size = hidden_size, \n",
    "                            batch_first = batch_first, \n",
    "                            bidirectional = True)\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
    "\n",
    "        self.gelu = nn.GELU()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.gelu(self.layer_norm(x))\n",
    "        x, _ = self.BiGRU(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class SpeechRecognitionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, num_classes, n_features, n_channels = 32, stride = 2, dropout_rate = 0.1):\n",
    "        \n",
    "        super(SpeechRecognitionModel, self).__init__()\n",
    "        \n",
    "        n_features = n_features // 2\n",
    "        \n",
    "        self.cnn = nn.Conv2d(in_channels = 1, \n",
    "                             out_channels = n_channels, \n",
    "                             kernel_size = 3, \n",
    "                             stride = stride, \n",
    "                             padding = 1)  \n",
    "\n",
    "        self.rcnn_layers = nn.Sequential(*[ResidualCNN(in_channels = n_channels, \n",
    "                                                       out_channels = n_channels, \n",
    "                                                       kernel_size = 3, \n",
    "                                                       stride = 1, \n",
    "                                                       dropout_rate = dropout_rate, \n",
    "                                                       n_features = n_features) \n",
    "                                           for _ in range(n_cnn_layers)])\n",
    "        \n",
    "        self.fully_connected = nn.Linear(n_channels*n_features, rnn_dim) \n",
    "        \n",
    "        self.brnn_layers = nn.Sequential(*[BidirectionalGRU(rnn_dim = (rnn_dim if i == 0 else rnn_dim*2),\n",
    "                                                            hidden_size = rnn_dim, \n",
    "                                                            dropout_rate = dropout_rate, \n",
    "                                                            batch_first = (i == 0))\n",
    "                                           for i in range(n_rnn_layers)])\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.Linear(rnn_dim*2, rnn_dim),\n",
    "                                        nn.GELU(),\n",
    "                                        nn.Dropout(dropout_rate),\n",
    "                                        nn.Linear(rnn_dim, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.cnn(x)                                        # (batch, feature, time)\n",
    "        x = self.rcnn_layers(x)                                # (batch, channels, feature, time)\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1]*sizes[2], sizes[3])      # (batch, channels * feature, time)\n",
    "        x = x.transpose(1, 2)                                  # (batch, time, channels * feature)\n",
    "        x = self.fully_connected(x)                            # (batch, time, rnn_dim)\n",
    "        x = self.brnn_layers(x)                                # (batch, time, 2 * rnn_dim)\n",
    "        x = self.classifier(x)                                 # (batch, time, num_classes)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGNhgc8H7Pdm"
   },
   "source": [
    "### Defining metrics for comparing the targets against the predictions\n",
    "\n",
    "The Levenshtein distance is a string metric for measuring the difference between two sequences. Informally, the Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other ([source](https://en.wikipedia.org/wiki/Levenshtein_distance#:~:text=Informally%2C%20the%20Levenshtein%20distance%20between,considered%20this%20distance%20in%201965.)).\n",
    "\n",
    "WER compares reference text and hypothesis text on word level by taking the ratio of the Levenshtein distance between the reference and the hypothesis at word level, and the number of words in the reference.\n",
    "\n",
    "CER compares reference text and hypothesis text on character level by taking the ratio of the Levenshtein distance between the reference and the hypothesis at character level, and the number of characters in the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iK5FCbET7Pdm"
   },
   "outputs": [],
   "source": [
    "def levenshtein_distance(reference, hypothesis):\n",
    "    \n",
    "    \"\"\"\n",
    "    Method for calculating Levenshtein distance between a pair of lists of elements\n",
    "    \"\"\"\n",
    "\n",
    "    if len(reference) > len(hypothesis):\n",
    "        reference, hypothesis = hypothesis, reference\n",
    "\n",
    "    edits = [[None]*(len(reference)+1), list(range(len(reference)+1))]\n",
    "\n",
    "    for i in range(1, len(hypothesis)+1):\n",
    "        edits = [edits[1], [0]*(len(reference)+1)]\n",
    "        edits[1][0] = i\n",
    "        for j in range(1, len(reference)+1):\n",
    "            if reference[j-1] != hypothesis[i-1]:\n",
    "                edits[1][j] = 1 + min(edits[0][j], edits[0][j-1], edits[1][j-1])\n",
    "            else:\n",
    "                edits[1][j] = edits[0][j-1]\n",
    "    \n",
    "    return edits[1][-1]\n",
    "\n",
    "\n",
    "def word_errors(reference, hypothesis, ignore_case = False, delimiter = ' '):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the Levenshtein distance between reference sequence and hypothesis sequence on word level.\n",
    "    \"\"\"\n",
    "\n",
    "    if ignore_case:\n",
    "        reference, hypothesis = reference.lower(), hypothesis.lower()\n",
    "\n",
    "    reference_words, hypothesis_words = reference.split(delimiter), hypothesis.split(delimiter)\n",
    "    lev_distance = levenshtein_distance(reference_words, hypothesis_words)\n",
    "\n",
    "    return lev_distance, len(reference_words)\n",
    "\n",
    "\n",
    "def character_errors(reference, hypothesis, ignore_case = False, remove_space = False):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the Levenshtein distance between reference sequence and hypothesis sequence on character level.\n",
    "    \"\"\"\n",
    "\n",
    "    if ignore_case:\n",
    "        reference, hypothesis = reference.lower(), hypothesis.lower()\n",
    "    if remove_space:\n",
    "        reference, hypothesis = reference.replace(\" \", \"\"), hypothesis.replace(\" \", \"\")\n",
    "\n",
    "    lev_distance = levenshtein_distance(reference, hypothesis)\n",
    "\n",
    "    return lev_distance, len(reference)\n",
    "\n",
    "\n",
    "def word_error_rate(reference, hypothesis, ignore_case = False, delimiter = ' '):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate word error rate (WER).\n",
    "    \"\"\"\n",
    "\n",
    "    lev_distance, reference_length = word_errors(reference, hypothesis, ignore_case, delimiter)\n",
    "    if reference_length == 0:\n",
    "        raise ValueError(\"Reference should not be an empty string. Hypothesis was \" + hypothesis)\n",
    "\n",
    "    return lev_distance / reference_length\n",
    "\n",
    "\n",
    "def character_error_rate(reference, hypothesis, ignore_case = False, remove_space = False):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate charactor error rate (CER).\n",
    "    \"\"\"\n",
    "\n",
    "    lev_distance, reference_length = character_errors(reference, hypothesis, ignore_case, remove_space)\n",
    "    if reference_length == 0:\n",
    "        raise ValueError(\"Reference should not be an empty string. Hypothesis was \" + hypothesis)\n",
    "    \n",
    "    return lev_distance / reference_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJ_dWi-77Pdn"
   },
   "source": [
    "### Function for decoding the output of the model\n",
    "The output of the model is in the form of a tensor of integers. This needs to be decoded into text, keeping in mind that repeats need to be ignored. The true labels are sliced to their original length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NdebK1fY7Pdn"
   },
   "outputs": [],
   "source": [
    "def decoder(output, labels, label_lengths, blank_label = 28, collapse_repeated = True):\n",
    "\n",
    "    # Calculate the character (class) with maximum probabiltiy, (batch, time, n_classes) -> (batch, time)\n",
    "    arg_maxs = torch.argmax(output, dim = 2)\n",
    "    \n",
    "    # Lists of decoded text and target text for all points in a batch\n",
    "    decoded_targets, decoded_predictions = [], []\n",
    "    \n",
    "    for label, label_length, args in zip(labels, label_lengths, arg_maxs):\n",
    "        \n",
    "        # Target text sliced to its original length (we padded each label to maximum length)\n",
    "        decoded_targets.append(text_transform.int_to_text(label[:label_length].tolist()))\n",
    "        \n",
    "        # List of integers to be decoded, result of using arg_max\n",
    "        decoded_int_list = []\n",
    "        for j in range(len(args)):\n",
    "            arg_max_0, arg_max_1 = args[j-1] if j != 0 else None, args[j]\n",
    "            if arg_max_1 != blank_label and (not collapse_repeated or j == 0 or arg_max_0 != arg_max_1):\n",
    "                decoded_int_list.append(arg_max_1.item())\n",
    "                \n",
    "        # Convert the list of integers to text, and the add it to the list of decoded text\n",
    "        decoded_prediction = text_transform.int_to_text(decoded_int_list)\n",
    "        decoded_predictions.append(decoded_prediction)\n",
    "  \n",
    "    return decoded_targets, decoded_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8t3nJT3ym_Yo"
   },
   "source": [
    "### Function for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zQhCcT9Um96E"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, scheduler):\n",
    "    \n",
    "    # Put the model in training mode\n",
    "    model.train()\n",
    "\n",
    "    # List of train loss\n",
    "    train_loss = []\n",
    "    \n",
    "    for data in tqdm(train_loader):\n",
    "        \n",
    "        # Load the data, and convert the tensor with the specified device\n",
    "        spectrograms, labels, input_lengths, label_lengths = data \n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(spectrograms)                              # (batch, time, num_classes)\n",
    "        output = F.log_softmax(output, dim = 2)\n",
    "        output = output.transpose(0, 1)                           # (time, batch, num_classes)\n",
    "\n",
    "        # Set the gradients to 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute the loss, and with it, the gradients\n",
    "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "        train_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate average character error rate and average word error rate\n",
    "    avg_train_loss = torch.mean(torch.tensor(train_loss))\n",
    "\n",
    "    print(\"Training set - Average loss = {:.4f}\".format(avg_train_loss))\n",
    "    time.sleep(2)\n",
    "    return avg_train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfbF7j6n7Pdn"
   },
   "source": [
    "### Function for testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "AymycgIt7Pdn"
   },
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion):\n",
    "    \n",
    "    # Put the model in testing mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize lists for test loss, character error rate and word error rate\n",
    "    test_loss, test_cer, test_wer = [], [], []\n",
    "    \n",
    "    # Don't compute gradients\n",
    "    with torch.no_grad():                     \n",
    "        \n",
    "        for data in tqdm(test_loader):\n",
    "            \n",
    "            # Load the data, and convert the tensor with the specified device\n",
    "            spectrograms, labels, input_lengths, label_lengths = data \n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(spectrograms)                              # (batch, time, num_classes)\n",
    "            output = F.log_softmax(output, dim = 2)\n",
    "            output = output.transpose(0, 1)                           # (time, batch, num_classes)\n",
    "\n",
    "            # Compute the loss, and add it to the total\n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            test_loss.append(loss.item())\n",
    "\n",
    "            # Decode targets and predictions\n",
    "            decoded_targets, decoded_predictions = decoder(output.transpose(0, 1), labels, label_lengths)\n",
    "            \n",
    "            # Calculate character error rate and word error rate\n",
    "            test_cer += [character_error_rate(decoded_target, decoded_prediction) for decoded_target, decoded_prediction in zip(decoded_targets, decoded_predictions)] \n",
    "            test_wer += [word_error_rate(decoded_target, decoded_prediction) for decoded_target, decoded_prediction in zip(decoded_targets, decoded_predictions)] \n",
    "\n",
    "    # Calculate average character error rate and average word error rate\n",
    "    avg_test_loss = torch.mean(torch.tensor(test_loss))\n",
    "    avg_test_cer = torch.mean(torch.tensor(test_cer))\n",
    "    avg_test_wer = torch.mean(torch.tensor(test_wer))\n",
    "\n",
    "    print(\"Validation set - Average loss = {:.4f}, Average CER = {:.4f}, Average WER = {:.4f}\".format(avg_test_loss, avg_test_cer, avg_test_wer))\n",
    "    time.sleep(2)\n",
    "    return avg_test_loss, avg_test_cer, avg_test_wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmFa2wbu7Pdo"
   },
   "source": [
    "### Bringing it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rV2PslEK7Pdo"
   },
   "outputs": [],
   "source": [
    "def main(learning_rate = 5e-4, batch_size = 16, epochs = 15, train_url = \"train-clean-100\", test_url = \"test-clean\"):\n",
    "\n",
    "    # Defining hyperparameters\n",
    "    hparams = {\"n_cnn_layers\": 3,\n",
    "               \"n_rnn_layers\": 5,\n",
    "               \"rnn_dim\": 512,\n",
    "               \"num_classes\": 29,\n",
    "               \"n_features\": 128,\n",
    "               \"stride\": 2,\n",
    "               \"dropout_rate\": 0.1,\n",
    "               \"learning_rate\": learning_rate,\n",
    "               \"batch_size\": batch_size,\n",
    "               \"epochs\": epochs}\n",
    "\n",
    "    # If current directory is not data, make one called data\n",
    "    if not os.path.isdir(\"./data\"):\n",
    "        os.makedirs(\"./data\")\n",
    "\n",
    "    # Downloading datasets\n",
    "    print(\"Downloading datasets\")\n",
    "    train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url = train_url, download = True)\n",
    "    test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url = test_url, download = True)\n",
    "    \n",
    "    # kwargs for using GPU\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    print(\"Using GPU\"  if use_cuda else \"GPU not found\")\n",
    "    kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}\n",
    "    \n",
    "    # Loading train data\n",
    "    print(\"Loading train data\")\n",
    "    train_loader = data.DataLoader(dataset = train_dataset,\n",
    "                                   batch_size = hparams[\"batch_size\"],\n",
    "                                   shuffle = True,\n",
    "                                   collate_fn = lambda x: data_processing(x, \"train\"),\n",
    "                                   **kwargs)\n",
    "    \n",
    "    # Loading test data\n",
    "    print(\"Loading test data\")\n",
    "    test_loader = data.DataLoader(dataset = test_dataset,\n",
    "                                 batch_size = hparams[\"batch_size\"],\n",
    "                                 shuffle = False,\n",
    "                                 collate_fn = lambda x: data_processing(x, \"validation\"),\n",
    "                                 **kwargs)\n",
    "\n",
    "    # Setting up the model\n",
    "    print(\"Setting up the model\")\n",
    "    model = SpeechRecognitionModel(n_cnn_layers = hparams[\"n_cnn_layers\"], \n",
    "                                   n_rnn_layers = hparams[\"n_rnn_layers\"], \n",
    "                                   rnn_dim = hparams[\"rnn_dim\"],\n",
    "                                   num_classes = hparams[\"num_classes\"], \n",
    "                                   n_features = hparams[\"n_features\"], \n",
    "                                   stride = hparams[\"stride\"], \n",
    "                                   dropout_rate = hparams[\"dropout_rate\"]).to(device)\n",
    "\n",
    "    print(\"Total model parameters =\", sum([param.nelement() for param in model.parameters()]))\n",
    "\n",
    "    # Optimizer, loss criterion, and learning rate scheduler\n",
    "    print(\"Defining optimizer, loss criterion and learning rate scheduler\")\n",
    "    optimizer = optim.AdamW(model.parameters(), hparams[\"learning_rate\"])\n",
    "    criterion = nn.CTCLoss(blank = 28).to(device)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "                                              max_lr = hparams[\"learning_rate\"], \n",
    "                                              steps_per_epoch = len(train_loader),\n",
    "                                              epochs = hparams[\"epochs\"],\n",
    "                                              anneal_strategy = \"linear\")\n",
    "    \n",
    "    # Training the model\n",
    "    print(\"\\nTraining\")\n",
    "    train_stats = {\"Train Losses\": []}\n",
    "    test_stats = {\"Test Losses\": [], \"Test CER\": [], \"Test WER\": []}\n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(\"\\nEpoch\", epoch)\n",
    "        time.sleep(2)\n",
    "        train_loss = train(model, device, train_loader, criterion, optimizer, scheduler)\n",
    "        test_loss, test_cer, test_wer = test(model, device, test_loader, criterion)\n",
    "        train_stats[\"Train Losses\"].append(train_loss)\n",
    "        test_stats[\"Test Losses\"].append(test_loss)\n",
    "        test_stats[\"Test CER\"].append(test_cer)\n",
    "        test_stats[\"Test WER\"].append(test_wer)\n",
    "\n",
    "    return model, train_stats, test_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "YIrKaW7_oufG"
   },
   "outputs": [],
   "source": [
    "def plot(train_loss, test_loss, xlabel, ylabel, title, labelpad=12, titlepad=12,\n",
    "         fontdict={\"family\":\"serif\", \"color\":\"darkred\", \"weight\":\"normal\", \"size\":16}, grid=True):\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.xlabel(xlabel, fontdict=fontdict, labelpad=labelpad)\n",
    "    plt.ylabel(ylabel, fontdict=fontdict, labelpad=labelpad)\n",
    "    plt.title(title, fontdict=fontdict, pad=titlepad)\n",
    "    plt.plot(train_loss, color=\"g\", label=\"Training\")\n",
    "    plt.plot(test_loss, color=\"b\", label=\"Validation\")\n",
    "    plt.legend()\n",
    "    if grid:\n",
    "        plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d6oMI4SF7Pdo",
    "outputId": "434ab97f-c936-496b-ffa3-9474f92b3f81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading datasets\n",
      "Using GPU\n",
      "Loading train data\n",
      "Loading test data\n",
      "Setting up the model\n",
      "Total model parameters = 23704989\n",
      "Defining optimizer, loss criterion and lr scheduler\n",
      "\n",
      "Training\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3568/3568 [27:27<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set - Average loss = 2.5082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 328/328 [02:18<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set - Average loss = 1.5269, Average CER = 0.4957, Average WER = 0.9882\n",
      "\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3568/3568 [27:32<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set - Average loss = 1.4619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 328/328 [02:17<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set - Average loss = 1.0712, Average CER = 0.3278, Average WER = 0.8152\n",
      "\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3568/3568 [27:30<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set - Average loss = 1.1932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 328/328 [02:20<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set - Average loss = 0.9324, Average CER = 0.2839, Average WER = 0.7445\n",
      "\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3568/3568 [27:29<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set - Average loss = 1.0515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 328/328 [02:21<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set - Average loss = 0.8288, Average CER = 0.2688, Average WER = 0.6770\n",
      "\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3568/3568 [27:31<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set - Average loss = 0.9701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 328/328 [02:20<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set - Average loss = 0.7408, Average CER = 0.2241, Average WER = 0.6150\n",
      "\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3568/3568 [27:31<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set - Average loss = 0.8819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 328/328 [02:22<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set - Average loss = 0.6882, Average CER = 0.2088, Average WER = 0.5856\n",
      "\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3568/3568 [27:30<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set - Average loss = 0.8117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 328/328 [02:22<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set - Average loss = 0.6104, Average CER = 0.1835, Average WER = 0.5338\n",
      "\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3568/3568 [27:29<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set - Average loss = 0.7536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 328/328 [02:22<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set - Average loss = 0.5820, Average CER = 0.1729, Average WER = 0.5062\n",
      "\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3568/3568 [27:33<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set - Average loss = 0.7132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 328/328 [02:22<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set - Average loss = 0.5442, Average CER = 0.1620, Average WER = 0.4825\n",
      "\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3568/3568 [27:33<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set - Average loss = 0.6610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 328/328 [02:17<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set - Average loss = 0.5146, Average CER = 0.1502, Average WER = 0.4517\n",
      "\n",
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3568/3568 [27:26<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set - Average loss = 0.6265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 328/328 [02:21<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set - Average loss = 0.4982, Average CER = 0.1469, Average WER = 0.4431\n",
      "\n",
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3568/3568 [27:30<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set - Average loss = 0.5941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 328/328 [02:25<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set - Average loss = 0.4854, Average CER = 0.1417, Average WER = 0.4279\n",
      "\n",
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3568/3568 [27:31<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set - Average loss = 0.5633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 328/328 [02:20<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set - Average loss = 0.4703, Average CER = 0.1352, Average WER = 0.4118\n",
      "\n",
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3568/3568 [27:31<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set - Average loss = 0.5430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 328/328 [02:20<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set - Average loss = 0.4588, Average CER = 0.1330, Average WER = 0.4062\n",
      "\n",
      "Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3568/3568 [27:31<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set - Average loss = 0.5216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 328/328 [02:20<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set - Average loss = 0.4554, Average CER = 0.1301, Average WER = 0.3981\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 5e-4\n",
    "batch_size = 8\n",
    "epochs = 15\n",
    "libri_train_set = \"train-clean-100\"\n",
    "libri_test_set = \"test-clean\"\n",
    "\n",
    "model, train_stats, test_stats = main(learning_rate = learning_rate, \n",
    "                                      batch_size = batch_size, \n",
    "                                      epochs = epochs, \n",
    "                                      train_url = libri_train_set, \n",
    "                                      test_url = libri_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "54-eYnWSsTmO"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_state_dict.pt\")\n",
    "torch.save(model, \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "yCRzYhySzlUN",
    "outputId": "7dc968b8-3bd0-4afe-de9f-6cf83a015282"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(train_loss = train_stats[\"Train Losses\"], \n",
    "     test_loss = test_stats[\"Test Losses\"], \n",
    "     xlabel = \"Epoch\", \n",
    "     ylabel = \"Loss\",\n",
    "     title = \"Training Loss and Test Loss\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Speech Recognition.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bitfd92962200d74e8ba4e1089743d69310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
